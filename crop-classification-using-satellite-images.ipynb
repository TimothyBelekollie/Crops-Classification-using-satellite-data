{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":93654,"databundleVersionId":11155209,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install xgboost --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, f1_score\nimport xgboost as xgb  # Import XGBoost\n\nimport ast  #Import the ast library","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:53:23.688887Z","iopub.execute_input":"2025-03-05T18:53:23.689218Z","iopub.status.idle":"2025-03-05T18:53:23.694608Z","shell.execute_reply.started":"2025-03-05T18:53:23.689189Z","shell.execute_reply":"2025-03-05T18:53:23.693190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Data (Update file paths as needed)\ntrain_data = pd.read_csv('/kaggle/input/ml-4-eo-s-2025-crop-classification-challenge/train.csv', sep=',')\ntest_data= pd.read_csv('/kaggle/input/ml-4-eo-s-2025-crop-classification-challenge/test.csv', sep=',')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:53:26.974966Z","iopub.execute_input":"2025-03-05T18:53:26.975329Z","iopub.status.idle":"2025-03-05T18:53:27.026903Z","shell.execute_reply.started":"2025-03-05T18:53:26.975283Z","shell.execute_reply":"2025-03-05T18:53:27.025681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:53:28.764123Z","iopub.execute_input":"2025-03-05T18:53:28.764499Z","iopub.status.idle":"2025-03-05T18:53:28.778793Z","shell.execute_reply.started":"2025-03-05T18:53:28.764471Z","shell.execute_reply":"2025-03-05T18:53:28.777559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:53:31.777100Z","iopub.execute_input":"2025-03-05T18:53:31.777476Z","iopub.status.idle":"2025-03-05T18:53:31.793021Z","shell.execute_reply.started":"2025-03-05T18:53:31.777442Z","shell.execute_reply":"2025-03-05T18:53:31.791861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\nimport pandas as pd\n\n# Assuming train_data and test_data are already defined\ntrain_df = train_data.copy()\ntest_df = test_data.copy()\n\n# Function to safely evaluate string representations of lists\ndef safe_eval(x):\n    try:\n        return ast.literal_eval(x)  # Convert string list to actual list\n    except:\n        return x  # Return original value if conversion fails\n\n# Function to process the DataFrame\ndef process_dataframe(df):\n    for col in df.columns:\n        df[col] = df[col].apply(safe_eval)\n    return df\n\n# Process training and test data\ntrain_df_processed = process_dataframe(train_df)\ntest_df_processed = process_dataframe(test_df)\n\n# Check for columns that still have lists (nested data)\nlist_columns = [col for col in train_df_processed.columns if isinstance(train_df_processed[col].iloc[0], list)]\nprint(f\"Columns containing lists: {list_columns}\")\n\n# Flatten list columns (expand into multiple columns)\ndef expand_list_columns(df, list_columns):\n    for col in list_columns:\n        max_length = max(df[col].apply(lambda x: len(x) if isinstance(x, list) else 0))  # Find max list length\n        for i in range(max_length):\n            df[f\"{col}_{i+1}\"] = df[col].apply(lambda x: x[i] if isinstance(x, list) and len(x) > i else None)\n        df.drop(columns=[col], inplace=True)  # Drop original list column\n    return df\n\n# Apply expansion to train and test data\ntrain_df_final = expand_list_columns(train_df_processed, list_columns)\ntest_df_final = expand_list_columns(test_df_processed, list_columns)\n\n# Verify the processed DataFrames\nprint(\"\\nProcessed training data (after expansion):\")\nprint(train_df_final.head())\nprint(train_df_final.dtypes)\n\nprint(\"\\nProcessed test data (after expansion):\")\nprint(test_df_final.head())\nprint(test_df_final.dtypes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:53:36.830721Z","iopub.execute_input":"2025-03-05T18:53:36.831035Z","iopub.status.idle":"2025-03-05T18:53:37.890592Z","shell.execute_reply.started":"2025-03-05T18:53:36.831009Z","shell.execute_reply":"2025-03-05T18:53:37.889737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values in both datasets\nprint(\"Missing values in training data:\")\nprint(train_df_final.isnull().sum().sum())\n\nprint(\"\\nMissing values in test data:\")\nprint(test_df_final.isnull().sum().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:53:53.205294Z","iopub.execute_input":"2025-03-05T18:53:53.205684Z","iopub.status.idle":"2025-03-05T18:53:53.220184Z","shell.execute_reply.started":"2025-03-05T18:53:53.205653Z","shell.execute_reply":"2025-03-05T18:53:53.218791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Columns with missing values in test data:\")\nprint(test_df_final.isnull().sum()[test_df_final.isnull().sum() > 0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:54:24.132727Z","iopub.execute_input":"2025-03-05T18:54:24.133052Z","iopub.status.idle":"2025-03-05T18:54:24.146135Z","shell.execute_reply.started":"2025-03-05T18:54:24.133026Z","shell.execute_reply":"2025-03-05T18:54:24.145155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df_final[['EVI_1', 'EVI_2', 'EVI_3', 'EVI_4']] = test_df_final[['EVI_1', 'EVI_2', 'EVI_3', 'EVI_4']].fillna(test_df_final.mean())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:55:31.878559Z","iopub.execute_input":"2025-03-05T18:55:31.878885Z","iopub.status.idle":"2025-03-05T18:55:31.891925Z","shell.execute_reply.started":"2025-03-05T18:55:31.878858Z","shell.execute_reply":"2025-03-05T18:55:31.890668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Columns with missing values in test data:\")\nprint(test_df_final.isnull().sum()[test_df_final.isnull().sum() > 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:55:36.170357Z","iopub.execute_input":"2025-03-05T18:55:36.170717Z","iopub.status.idle":"2025-03-05T18:55:36.184013Z","shell.execute_reply.started":"2025-03-05T18:55:36.170667Z","shell.execute_reply":"2025-03-05T18:55:36.182916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nChecking for infinity values in test data:\")\nprint((test_df_final == np.inf).sum().sum() + (test_df_final == -np.inf).sum().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:55:46.220760Z","iopub.execute_input":"2025-03-05T18:55:46.221090Z","iopub.status.idle":"2025-03-05T18:55:46.235965Z","shell.execute_reply.started":"2025-03-05T18:55:46.221059Z","shell.execute_reply":"2025-03-05T18:55:46.234551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_df_final.replace([np.inf, -np.inf], 1e6, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:38:24.555633Z","iopub.execute_input":"2025-03-05T18:38:24.555944Z","iopub.status.idle":"2025-03-05T18:38:24.575069Z","shell.execute_reply.started":"2025-03-05T18:38:24.555917Z","shell.execute_reply":"2025-03-05T18:38:24.573724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df=test_df_final.copy()\ntrain_df=train_df_final.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:57:12.512188Z","iopub.execute_input":"2025-03-05T18:57:12.512563Z","iopub.status.idle":"2025-03-05T18:57:12.519221Z","shell.execute_reply.started":"2025-03-05T18:57:12.512537Z","shell.execute_reply":"2025-03-05T18:57:12.518569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Create DataFrame\n# train_df = pd.DataFrame(train_df)\n\n# # Process each column except crop_label\n# for col in train_df.columns:\n#     if col != 'crop_label':\n#         # Split the list in each cell and create new columns\n#         df_expanded = train_df[col].apply(pd.Series)\n#         df_expanded.columns = [f\"{col}_{i+1}\" for i in range(df_expanded.shape[1])]\n        \n#         # Drop original column with lists and join the new expanded columns\n#         train_df = pd.concat([train_df.drop(columns=[col]), df_expanded], axis=1)\n\n# # Display the final DataFrame\n# print(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:57:20.746421Z","iopub.execute_input":"2025-03-05T18:57:20.746749Z","iopub.status.idle":"2025-03-05T18:57:20.750750Z","shell.execute_reply.started":"2025-03-05T18:57:20.746724Z","shell.execute_reply":"2025-03-05T18:57:20.749385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Create DataFrame\n# test_df = pd.DataFrame(test_df)\n\n# # Process each column except 'field_id'\n# for col in test_df.columns:\n#     if col != 'field_id':  # Exclude field_id\n#         # Split the list in each cell and create new columns\n#         df_expanded = test_df[col].apply(pd.Series)\n#         df_expanded.columns = [f\"{col}_{i+1}\" for i in range(df_expanded.shape[1])]\n        \n#         # Drop original column with lists and join the new expanded columns\n#         test_df = pd.concat([test_df.drop(columns=[col]), df_expanded], axis=1)\n\n# # Display the final DataFrame\n# print(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:57:29.163617Z","iopub.execute_input":"2025-03-05T18:57:29.163983Z","iopub.status.idle":"2025-03-05T18:57:29.167509Z","shell.execute_reply.started":"2025-03-05T18:57:29.163952Z","shell.execute_reply":"2025-03-05T18:57:29.166634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n\n\n# Encode crop labels\nlabel_mapping = {\n    'Maize': 1,\n    'Sorghum': 2,\n    'Tree': 3,\n    'Built': 4,\n    'Bare_Soil': 5,\n    'Water': 6\n}\n\ntrain_df['encoded_label'] = train_df['crop_label'].map(label_mapping)\n\n# Print the encoded training dataframe\nprint(\"Encoded Training Data:\")\nprint(train_df)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:57:38.974064Z","iopub.execute_input":"2025-03-05T18:57:38.974427Z","iopub.status.idle":"2025-03-05T18:57:38.995294Z","shell.execute_reply.started":"2025-03-05T18:57:38.974400Z","shell.execute_reply":"2025-03-05T18:57:38.993622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.isna().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:57:51.193491Z","iopub.execute_input":"2025-03-05T18:57:51.193853Z","iopub.status.idle":"2025-03-05T18:57:51.202720Z","shell.execute_reply.started":"2025-03-05T18:57:51.193822Z","shell.execute_reply":"2025-03-05T18:57:51.201809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.isna().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:57:53.966130Z","iopub.execute_input":"2025-03-05T18:57:53.966488Z","iopub.status.idle":"2025-03-05T18:57:53.974039Z","shell.execute_reply.started":"2025-03-05T18:57:53.966460Z","shell.execute_reply":"2025-03-05T18:57:53.973095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Find rows and columns with null values\n# null_locations = test_df[test_df.isna().any(axis=1)]\n\n# # Display the rows with null values\n# print(\"Rows with Null Values:\")\n# print(null_locations)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:39:43.331628Z","iopub.execute_input":"2025-03-05T18:39:43.331930Z","iopub.status.idle":"2025-03-05T18:39:43.336111Z","shell.execute_reply.started":"2025-03-05T18:39:43.331905Z","shell.execute_reply":"2025-03-05T18:39:43.334697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Identify columns with null values for each row\n# null_columns = test_df.columns[test_df.isna().any()]\n\n# # Display columns with null values for each row\n# print(\"Columns with Null Values in Rows:\")\n# for column in null_columns:\n#     print(f\"Column: {column}\")\n#     print(test_df[test_df[column].isna()])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:39:51.158376Z","iopub.execute_input":"2025-03-05T18:39:51.158696Z","iopub.status.idle":"2025-03-05T18:39:51.162041Z","shell.execute_reply.started":"2025-03-05T18:39:51.158670Z","shell.execute_reply":"2025-03-05T18:39:51.161125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:57:57.910006Z","iopub.execute_input":"2025-03-05T18:57:57.910471Z","iopub.status.idle":"2025-03-05T18:57:57.916968Z","shell.execute_reply.started":"2025-03-05T18:57:57.910447Z","shell.execute_reply":"2025-03-05T18:57:57.915373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:58:00.396757Z","iopub.execute_input":"2025-03-05T18:58:00.397052Z","iopub.status.idle":"2025-03-05T18:58:00.404003Z","shell.execute_reply.started":"2025-03-05T18:58:00.397029Z","shell.execute_reply":"2025-03-05T18:58:00.402705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # For each column with null values, display the null values explicitly\n# for column in null_columns:\n#     print(f\"Null values in column: {column}\")\n#     print(test_df[test_df[column].isna()][column])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:48.168851Z","iopub.execute_input":"2025-03-05T18:40:48.169174Z","iopub.status.idle":"2025-03-05T18:40:48.177793Z","shell.execute_reply.started":"2025-03-05T18:40:48.169145Z","shell.execute_reply":"2025-03-05T18:40:48.176492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Replace positive and negative infinity with a large number (e.g., 1e6)\ntest_df.replace([np.inf, -np.inf], 1e6, inplace=True)\n\n# If you want to check that no inf values remain:\nprint(test_df.isin([np.inf, -np.inf]).sum().sum())  # Should print 0 if no more inf values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:58:14.767795Z","iopub.execute_input":"2025-03-05T18:58:14.768136Z","iopub.status.idle":"2025-03-05T18:58:14.781713Z","shell.execute_reply.started":"2025-03-05T18:58:14.768106Z","shell.execute_reply":"2025-03-05T18:58:14.780874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.isna().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:58:18.159205Z","iopub.execute_input":"2025-03-05T18:58:18.159613Z","iopub.status.idle":"2025-03-05T18:58:18.165908Z","shell.execute_reply.started":"2025-03-05T18:58:18.159587Z","shell.execute_reply":"2025-03-05T18:58:18.164966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # For each column with null values, display the null values explicitly\n# for column in null_columns:\n#     print(f\"Null values in column: {column}\")\n#     print(test_df[test_df[column].isna()][column])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:13:30.146920Z","iopub.execute_input":"2025-03-05T18:13:30.147119Z","iopub.status.idle":"2025-03-05T18:13:30.171157Z","shell.execute_reply.started":"2025-03-05T18:13:30.147101Z","shell.execute_reply":"2025-03-05T18:13:30.170000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Forward Fill (ffill) without using inplace=True\n# test_df['EVI_2'] = test_df['EVI_2'].ffill()\n# test_df['EVI_3'] = test_df['EVI_3'].ffill()\n# test_df['EVI_4'] = test_df['EVI_4'].ffill()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:13:30.174209Z","iopub.execute_input":"2025-03-05T18:13:30.174509Z","iopub.status.idle":"2025-03-05T18:13:30.197378Z","shell.execute_reply.started":"2025-03-05T18:13:30.174483Z","shell.execute_reply":"2025-03-05T18:13:30.195584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.isna().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:58:37.086046Z","iopub.execute_input":"2025-03-05T18:58:37.086506Z","iopub.status.idle":"2025-03-05T18:58:37.095282Z","shell.execute_reply.started":"2025-03-05T18:58:37.086475Z","shell.execute_reply":"2025-03-05T18:58:37.093933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:58:42.869728Z","iopub.execute_input":"2025-03-05T18:58:42.870008Z","iopub.status.idle":"2025-03-05T18:58:42.875753Z","shell.execute_reply.started":"2025-03-05T18:58:42.869985Z","shell.execute_reply":"2025-03-05T18:58:42.874457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the data types of each column\nprint(test_df.dtypes)\n\n# Identify columns with non-numeric data\nnon_numeric_columns = test_df.select_dtypes(exclude=['number']).columns\nprint(\"Non-numeric columns:\", non_numeric_columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:21:19.913714Z","iopub.execute_input":"2025-03-05T18:21:19.914036Z","iopub.status.idle":"2025-03-05T18:21:19.922174Z","shell.execute_reply.started":"2025-03-05T18:21:19.914009Z","shell.execute_reply":"2025-03-05T18:21:19.920692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(test_df['EVI_1'].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:59:05.824694Z","iopub.execute_input":"2025-03-05T18:59:05.825003Z","iopub.status.idle":"2025-03-05T18:59:05.831060Z","shell.execute_reply.started":"2025-03-05T18:59:05.824977Z","shell.execute_reply":"2025-03-05T18:59:05.829769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Replace any remaining inf values with a large number or NaN\n# test_df.replace([np.inf, -np.inf], 1e6, inplace=True)\n\n# # If you want to check that no inf values remain:\n# print(test_df.isin([np.inf, -np.inf]).sum().sum())  # Should print 0 if no more inf values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:25:09.276609Z","iopub.execute_input":"2025-03-05T18:25:09.276946Z","iopub.status.idle":"2025-03-05T18:25:09.291431Z","shell.execute_reply.started":"2025-03-05T18:25:09.276918Z","shell.execute_reply":"2025-03-05T18:25:09.290325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Convert the 'EVI_1' column to numeric, forcing errors to NaN (if any)\n# test_df['EVI_1'] = pd.to_numeric(test_df['EVI_1'], errors='coerce')\n\n# # Optionally, check for any NaNs after conversion\n# print(test_df['EVI_1'].isna().sum())  # Should be 0 if all values were successfully converted\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:25:12.435644Z","iopub.execute_input":"2025-03-05T18:25:12.435984Z","iopub.status.idle":"2025-03-05T18:25:12.443066Z","shell.execute_reply.started":"2025-03-05T18:25:12.435957Z","shell.execute_reply":"2025-03-05T18:25:12.441405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:59:42.499290Z","iopub.execute_input":"2025-03-05T18:59:42.499613Z","iopub.status.idle":"2025-03-05T18:59:42.520388Z","shell.execute_reply.started":"2025-03-05T18:59:42.499585Z","shell.execute_reply":"2025-03-05T18:59:42.518871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import StandardScaler\n\n# # Separate features (X) and target (y)\n# X = train_df.drop(columns=['crop_label', 'encoded_label'])\n# y = train_df['encoded_label']\n\n# # Split data into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n\n# # Standardize features\n# scaler = StandardScaler()\n# X_train_scaled = scaler.fit_transform(X_train)\n# X_test_scaled = scaler.transform(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:13:30.279558Z","iopub.execute_input":"2025-03-05T18:13:30.279858Z","iopub.status.idle":"2025-03-05T18:13:30.298705Z","shell.execute_reply.started":"2025-03-05T18:13:30.279828Z","shell.execute_reply":"2025-03-05T18:13:30.297190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# # Shift labels to start from 0\n# y = train_df['encoded_label'] - 1\n\n# # Split data into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n\n# # Standardize features\n# scaler = StandardScaler()\n# X_train_scaled = scaler.fit_transform(X_train)\n# X_test_scaled = scaler.transform(X_test)\n\n# # Initialize models\n# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n# xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n\n# # Train models\n# rf_model.fit(X_train_scaled, y_train)\n# xgb_model.fit(X_train_scaled, y_train)\n\n# # Make predictions\n# rf_predictions = rf_model.predict(X_test_scaled)\n# xgb_predictions = xgb_model.predict(X_test_scaled)\n\n# # Evaluate F1 score\n# rf_f1 = f1_score(y_test, rf_predictions, average='weighted')\n# xgb_f1 = f1_score(y_test, xgb_predictions, average='weighted')\n\n# print(f\"Random Forest F1 Score: {rf_f1}\")\n# print(f\"XGBoost F1 Score: {xgb_f1}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:13:30.299746Z","iopub.execute_input":"2025-03-05T18:13:30.300043Z","iopub.status.idle":"2025-03-05T18:13:30.320418Z","shell.execute_reply.started":"2025-03-05T18:13:30.300014Z","shell.execute_reply":"2025-03-05T18:13:30.319043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n# from sklearn.ensemble import RandomForestClassifier\n\n# # Define hyperparameters to tune\n# param_grid = {\n#     'n_estimators': [100, 200, 300],\n#     'max_depth': [None, 10, 20, 30],\n#     'min_samples_split': [2, 5, 10],\n#     'min_samples_leaf': [1, 2, 4],\n#     'max_features': ['auto', 'sqrt', 'log2'],\n#     'bootstrap': [True, False]\n# }\n\n# # Create Random Forest model\n# rf_model = RandomForestClassifier(random_state=42)\n\n# # Perform Grid Search\n# grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='f1_weighted', verbose=2, n_jobs=-1)\n\n# # Fit the model\n# grid_search.fit(X_train_scaled, y_train)\n\n# # Print the best parameters and the best score\n# print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n#     print(f\"Best F1 Score: {grid_search.best_score_}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:15:49.136910Z","iopub.execute_input":"2025-03-05T18:15:49.137285Z","iopub.status.idle":"2025-03-05T18:15:49.141683Z","shell.execute_reply.started":"2025-03-05T18:15:49.137221Z","shell.execute_reply":"2025-03-05T18:15:49.140636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n# from xgboost import XGBClassifier\n\n# # Define the model\n# xgb_model = XGBClassifier(objective=\"multi:softmax\", num_class=6, random_state=42)\n\n# # Define parameter grid for fine-tuning\n# param_grid = {\n#     \"n_estimators\": [150, 200, 250, 300],\n#     \"max_depth\": [5, 6, 7, 8],\n#     \"learning_rate\": [0.05, 0.1, 0.2],\n#     \"colsample_bytree\": [0.7, 0.8, 0.9],\n#     \"gamma\": [0, 0.1, 0.2],\n#     \"scale_pos_weight\": [1]  # No severe class imbalance\n# }\n\n# # Grid search with 5-fold cross-validation\n# grid_search = GridSearchCV(\n#     xgb_model, param_grid, scoring=\"f1_weighted\", cv=5, verbose=2, n_jobs=-1\n# )\n\n# # Fit the model (this will take time)\n# grid_search.fit(X_train_scaled, y_train)\n\n# # Get the best parameters and F1 score\n# best_params = grid_search.best_params_\n# best_f1_score = grid_search.best_score_\n\n# print(\"Best Hyperparameters:\", best_params)\n# print(\"Best F1 Score:\", best_f1_score)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:15:55.621596Z","iopub.execute_input":"2025-03-05T18:15:55.621937Z","iopub.status.idle":"2025-03-05T18:15:55.626601Z","shell.execute_reply.started":"2025-03-05T18:15:55.621905Z","shell.execute_reply":"2025-03-05T18:15:55.625343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:59:52.774560Z","iopub.execute_input":"2025-03-05T18:59:52.774869Z","iopub.status.idle":"2025-03-05T18:59:52.781683Z","shell.execute_reply.started":"2025-03-05T18:59:52.774843Z","shell.execute_reply":"2025-03-05T18:59:52.780580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:59:57.508985Z","iopub.execute_input":"2025-03-05T18:59:57.509401Z","iopub.status.idle":"2025-03-05T18:59:57.530153Z","shell.execute_reply.started":"2025-03-05T18:59:57.509368Z","shell.execute_reply":"2025-03-05T18:59:57.528858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.ensemble import VotingClassifier\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import StandardScaler\n# import pandas as pd\n# import xgboost as xgb\n# from sklearn.ensemble import RandomForestClassifier\n\n# # Prepare the test data (after training the models)\n# X_test_final = test_df.drop(columns=['field_id'])  # Drop non-feature columns\n# X_test_scaled = scaler.transform(X_test_final)  # Apply the same scaler used on the training data\n\n# # Initialize the models\n# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n# xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n\n# # Create a voting classifier with hard voting\n# voting_clf = VotingClassifier(estimators=[('rf', rf_model), ('xgb', xgb_model)], voting='hard')\n\n# # Train the voting classifier (you can skip this if you already have the trained models)\n# voting_clf.fit(X_train_scaled, y_train)\n\n# # Make predictions with the voting classifier\n# voting_predictions = voting_clf.predict(X_test_scaled)\n\n# # Alternatively, if you want to use pre-trained models (without training the voting model):\n# rf_predictions = rf_model.predict(X_test_scaled)\n# xgb_predictions = xgb_model.predict(X_test_scaled)\n\n# # Perform majority voting (hard voting)\n# final_predictions = [max(set([rf, xgb]), key=[rf, xgb].count) for rf, xgb in zip(rf_predictions, xgb_predictions)]\n\n# # Create the submission DataFrame\n# submission_df = pd.DataFrame({\n#     'ID': test_df['field_id'],\n#     'label': final_predictions  # The final predicted labels after voting\n# })\n\n# # Save the submission file\n# submission_df.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:16:13.691505Z","iopub.execute_input":"2025-03-05T18:16:13.691868Z","iopub.status.idle":"2025-03-05T18:16:13.695960Z","shell.execute_reply.started":"2025-03-05T18:16:13.691837Z","shell.execute_reply":"2025-03-05T18:16:13.695058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# # Hyperparameters for tuning RandomForest\n# rf_param_grid = {\n#     'n_estimators': [100, 200],\n#     'max_depth': [None, 10, 20],\n#     'min_samples_split': [2, 5],\n#     'min_samples_leaf': [1, 2],\n# }\n\n# # Hyperparameters for tuning XGBoost\n# xgb_param_grid = {\n#     'n_estimators': [100, 200],\n#     'learning_rate': [0.01, 0.1],\n#     'max_depth': [3, 6],\n# }\n\n# # Initialize models\n# rf_model = RandomForestClassifier(random_state=42)\n# xgb_model = xgb.XGBClassifier(random_state=42)\n\n# # Perform grid search for RandomForest\n# rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='f1_weighted', verbose=2, n_jobs=-1)\n# rf_grid_search.fit(X_train_scaled, y_train)\n\n# # Perform grid search for XGBoost\n# xgb_grid_search = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='f1_weighted', verbose=2, n_jobs=-1)\n# xgb_grid_search.fit(X_train_scaled, y_train)\n\n# # Get the best models after tuning\n# best_rf_model = rf_grid_search.best_estimator_\n# best_xgb_model = xgb_grid_search.best_estimator_\n\n# # Combine these tuned models into a voting classifier\n# voting_clf = VotingClassifier(estimators=[('rf', best_rf_model), ('xgb', best_xgb_model)], voting='hard')\n\n# # Train the voting classifier\n# voting_clf.fit(X_train_scaled, y_train)\n\n# # Make predictions with the voting classifier\n# voting_predictions = voting_clf.predict(X_test_scaled)\n\n# # Create the submission DataFrame\n# submission_df = pd.DataFrame({\n#     'field_id': test_df['field_id'],\n#     'encoded_label': voting_predictions  # The final predicted labels after voting\n# })\n\n# # Save the submission file\n# submission_df.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:16:28.293797Z","iopub.execute_input":"2025-03-05T18:16:28.294134Z","iopub.status.idle":"2025-03-05T18:16:28.298989Z","shell.execute_reply.started":"2025-03-05T18:16:28.294106Z","shell.execute_reply":"2025-03-05T18:16:28.297733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:01:07.101921Z","iopub.execute_input":"2025-03-05T19:01:07.102278Z","iopub.status.idle":"2025-03-05T19:01:07.107989Z","shell.execute_reply.started":"2025-03-05T19:01:07.102247Z","shell.execute_reply":"2025-03-05T19:01:07.106669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:01:09.947079Z","iopub.execute_input":"2025-03-05T19:01:09.947534Z","iopub.status.idle":"2025-03-05T19:01:09.968575Z","shell.execute_reply.started":"2025-03-05T19:01:09.947502Z","shell.execute_reply":"2025-03-05T19:01:09.967396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# # Load your training and test datasets\n# train_df = pd.read_csv('train.csv')  # Make sure to load your actual data\n# test_df = pd.read_csv('test.csv')  # Similarly, load your test data for submission\n\n# Separate features (X) and target (y) from the training data\nX = train_df.drop(columns=['crop_label', 'encoded_label'])  # Dropping non-feature columns\ny = train_df['encoded_label']\n\n# Split data into train and test sets for model validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n\n# Standardize features using StandardScaler (only fit on training data)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)  # Fit and transform the training data\nX_test_scaled = scaler.transform(X_test)  # Transform the test set using the same scaler\n\n# Prepare the test data (test_df) for final predictions (field_id is not a feature)\nX_test_final = test_df.drop(columns=['field_id'])  # Drop non-feature columns (ID column)\nX_test_final_scaled = scaler.transform(X_test_final)  # Transform the external test dataset (test_df)\n\n# Define the best parameter grids for Random Forest and XGBoost\n\n# Random Forest Hyperparameters\nrf_param_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'bootstrap': [True, False]\n}\n\n# XGBoost Hyperparameters\nxgb_param_grid = {\n    \"n_estimators\": [150, 200, 250, 300],\n    \"max_depth\": [5, 6, 7, 8],\n    \"learning_rate\": [0.05, 0.1, 0.2],\n    \"colsample_bytree\": [0.7, 0.8, 0.9],\n    \"gamma\": [0, 0.1, 0.2],\n    \"scale_pos_weight\": [1]  # No severe class imbalance\n}\n\n# Initialize models\nrf_model = RandomForestClassifier(random_state=42)\nxgb_model = xgb.XGBClassifier(random_state=42)\n\n# Perform GridSearchCV for RandomForest\nrf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='f1_weighted', verbose=2, n_jobs=-1)\nrf_grid_search.fit(X_train_scaled, y_train)\n\n# Perform GridSearchCV for XGBoost\nxgb_grid_search = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='f1_weighted', verbose=2, n_jobs=-1)\nxgb_grid_search.fit(X_train_scaled, y_train)\n\n# Get the best models after tuning\nbest_rf_model = rf_grid_search.best_estimator_\nbest_xgb_model = xgb_grid_search.best_estimator_\n\n# Create a Voting Classifier using the best models from GridSearchCV\nvoting_clf = VotingClassifier(estimators=[('rf', best_rf_model), ('xgb', best_xgb_model)], voting='hard')\n\n# Train the voting classifier on the training data\nvoting_clf.fit(X_train_scaled, y_train)\n\n# Make predictions with the voting classifier on the test data\nvoting_predictions = voting_clf.predict(X_test_final_scaled)\n\n# Create the submission DataFrame\nsubmission_df = pd.DataFrame({\n    'field_id': test_df['field_id'],  # Extract 'field_id' from test_df to keep track of the rows\n    'encoded_label': voting_predictions  # The final predicted labels after voting\n})\n\n# Save the submission file\nsubmission_df.to_csv('submission1.csv', index=False)\n\n# Optionally print the best parameters and scores for both models\nprint(f\"Best Random Forest Hyperparameters: {rf_grid_search.best_params_}\")\nprint(f\"Best XGBoost Hyperparameters: {xgb_grid_search.best_params_}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:01:24.007848Z","iopub.execute_input":"2025-03-05T19:01:24.008214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}